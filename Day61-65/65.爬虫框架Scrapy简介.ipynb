{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b318aef",
   "metadata": {},
   "source": [
    "## 爬虫框架Scrapy简介\n",
    "\n",
    "当你写了很多个爬虫程序之后，你会发现每次写爬虫程序时，都需要将页面获取、页面解析、爬虫调度、异常处理、反爬应对这些代码从头至尾实现一遍，这里面有很多工作其实都是简单乏味的重复劳动。那么，有没有什么办法可以提升我们编写爬虫代码的效率呢？答案是肯定的，那就是利用爬虫框架，而在所有的爬虫框架中，Scrapy 应该是最流行、最强大的框架。\n",
    "\n",
    "### Scrapy 概述\n",
    "\n",
    "Scrapy 是基于 Python 的一个非常流行的网络爬虫框架，可以用来抓取 Web 站点并从页面中提取结构化的数据。下图展示了 Scrapy 的基本架构，其中包含了主要组件和系统的数据处理流程（图中带数字的红色箭头）。\n",
    "\n",
    "<img src=\"res/20210824003638.png\" style=\"zoom:50%;\">\n",
    "\n",
    "#### Scrapy的组件\n",
    "\n",
    "我们先来说说 Scrapy 中的组件。\n",
    "\n",
    "1. Scrapy 引擎（Engine）：用来控制整个系统的数据处理流程。\n",
    "2. 调度器（Scheduler）：调度器从引擎接受请求并排序列入队列，并在引擎发出请求后返还给它们。\n",
    "3. 下载器（Downloader）：下载器的主要职责是抓取网页并将网页内容返还给蜘蛛（Spiders）。\n",
    "4. 蜘蛛程序（Spiders）：蜘蛛是用户自定义的用来解析网页并抓取特定URL的类，每个蜘蛛都能处理一个域名或一组域名，简单的说就是用来定义特定网站的抓取和解析规则的模块。\n",
    "5. 数据管道（Item Pipeline）：管道的主要责任是负责处理有蜘蛛从网页中抽取的数据条目，它的主要任务是清理、验证和存储数据。当页面被蜘蛛解析后，将被发送到数据管道，并经过几个特定的次序处理数据。每个数据管道组件都是一个 Python 类，它们获取了数据条目并执行对数据条目进行处理的方法，同时还需要确定是否需要在数据管道中继续执行下一步或是直接丢弃掉不处理。数据管道通常执行的任务有：清理 HTML 数据、验证解析到的数据（检查条目是否包含必要的字段）、检查是不是重复数据（如果重复就丢弃）、将解析到的数据存储到数据库（关系型数据库或 NoSQL 数据库）中。\n",
    "6. 中间件（Middlewares）：中间件是介于引擎和其他组件之间的一个钩子框架，主要是为了提供自定义的代码来拓展 Scrapy 的功能，包括下载器中间件和蜘蛛中间件。\n",
    "\n",
    "#### 数据处理流程\n",
    "\n",
    "Scrapy 的整个数据处理流程由引擎进行控制，通常的运转流程包括以下的步骤：\n",
    "\n",
    "1. 引擎询问蜘蛛需要处理哪个网站，并让蜘蛛将第一个需要处理的 URL 交给它。\n",
    "\n",
    "2. 引擎让调度器将需要处理的 URL 放在队列中。\n",
    "\n",
    "3. 引擎从调度那获取接下来进行爬取的页面。\n",
    "\n",
    "4. 调度将下一个爬取的 URL 返回给引擎，引擎将它通过下载中间件发送到下载器。\n",
    "\n",
    "5. 当网页被下载器下载完成以后，响应内容通过下载中间件被发送到引擎；如果下载失败了，引擎会通知调度器记录这个 URL，待会再重新下载。\n",
    "\n",
    "6. 引擎收到下载器的响应并将它通过蜘蛛中间件发送到蜘蛛进行处理。\n",
    "\n",
    "7. 蜘蛛处理响应并返回爬取到的数据条目，此外还要将需要跟进的新的 URL 发送给引擎。\n",
    "\n",
    "8. 引擎将抓取到的数据条目送入数据管道，把新的 URL 发送给调度器放入队列中。\n",
    "\n",
    "上述操作中的第2步到第8步会一直重复直到调度器中没有需要请求的 URL，爬虫就停止工作。\n",
    "\n",
    "### 安装和使用Scrapy\n",
    "\n",
    "可以使用 Python 的包管理工具`pip`来安装 Scrapy。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75652618",
   "metadata": {
    "attributes": {
     "classes": [
      "Shell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6300a7",
   "metadata": {},
   "source": [
    "在命令行中使用`scrapy`命令创建名为`demo`的项目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a473383",
   "metadata": {
    "attributes": {
     "classes": [
      "Bash"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "scrapy startproject demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e2b243",
   "metadata": {},
   "source": [
    "项目的目录结构如下图所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05b52ad",
   "metadata": {
    "attributes": {
     "classes": [
      "Shell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "demo\n",
    "|____ demo\n",
    "|________ spiders\n",
    "|____________ __init__.py\n",
    "|________ __init__.py\n",
    "|________ items.py\n",
    "|________ middlewares.py\n",
    "|________ pipelines.py\n",
    "|________ settings.py\n",
    "|____ scrapy.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777f5c4c",
   "metadata": {},
   "source": [
    "切换到`demo` 目录，用下面的命令创建名为`douban`的蜘蛛程序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa36b0",
   "metadata": {
    "attributes": {
     "classes": [
      "Bash"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "scrapy genspider douban movie.douban.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c233b4",
   "metadata": {},
   "source": [
    "#### 一个简单的例子\n",
    "\n",
    "接下来，我们实现一个爬取豆瓣电影 Top250 电影标题、评分和金句的爬虫。\n",
    "\n",
    "1. 在`items.py`的`Item`类中定义字段，这些字段用来保存数据，方便后续的操作。\n",
    "\n",
    "   ```Python\n",
    "   import scrapy\n",
    "   \n",
    "   \n",
    "   class DoubanItem(scrapy.Item):\n",
    "       title = scrapy.Field()\n",
    "       score = scrapy.Field()\n",
    "       motto = scrapy.Field()\n",
    "   ```\n",
    "   \n",
    "2. 修改`spiders`文件夹中名为`douban.py` 的文件，它是蜘蛛程序的核心，需要我们添加解析页面的代码。在这里，我们可以通过对`Response`对象的解析，获取电影的信息，代码如下所示。\n",
    "\n",
    "   ```Python\n",
    "   import scrapy\n",
    "   from scrapy import Selector, Request\n",
    "   from scrapy.http import HtmlResponse\n",
    "   \n",
    "   from demo.items import MovieItem\n",
    "   \n",
    "   \n",
    "   class DoubanSpider(scrapy.Spider):\n",
    "       name = 'douban'\n",
    "       allowed_domains = ['movie.douban.com']\n",
    "       start_urls = ['https://movie.douban.com/top250?start=0&filter=']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba37a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response: HtmlResponse):\n",
    "    sel = Selector(response)\n",
    "    movie_items = sel.css('#content > div > div.article > ol > li')\n",
    "    for movie_sel in movie_items:\n",
    "        item = MovieItem()\n",
    "        item['title'] = movie_sel.css('.title::text').extract_first()\n",
    "        item['score'] = movie_sel.css('.rating_num::text').extract_first()\n",
    "        item['motto'] = movie_sel.css('.inq::text').extract_first()\n",
    "        yield item\n",
    "   ```\n",
    "   通过上面的代码不难看出，我们可以使用 CSS 选择器进行页面解析。当然，如果你愿意也可以使用 XPath 或正则表达式进行页面解析，对应的方法分别是`xpath`和`re`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26e3228",
   "metadata": {},
   "source": [
    "如果还要生成后续爬取的请求，我们可以用`yield`产出`Request`对象。`Request`对象有两个非常重要的属性，一个是`url`，它代表了要请求的地址；一个是`callback`，它代表了获得响应之后要执行的回调函数。我们可以将上面的代码稍作修改。\n",
    "\n",
    "   ```Python\n",
    "   import scrapy\n",
    "   from scrapy import Selector, Request\n",
    "   from scrapy.http import HtmlResponse\n",
    "   \n",
    "   from demo.items import MovieItem\n",
    "   \n",
    "   \n",
    "   class DoubanSpider(scrapy.Spider):\n",
    "       name = 'douban'\n",
    "       allowed_domains = ['movie.douban.com']\n",
    "       start_urls = ['https://movie.douban.com/top250?start=0&filter=']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb4b992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response: HtmlResponse):\n",
    "    sel = Selector(response)\n",
    "    movie_items = sel.css('#content > div > div.article > ol > li')\n",
    "    for movie_sel in movie_items:\n",
    "        item = MovieItem()\n",
    "        item['title'] = movie_sel.css('.title::text').extract_first()\n",
    "        item['score'] = movie_sel.css('.rating_num::text').extract_first()\n",
    "        item['motto'] = movie_sel.css('.inq::text').extract_first()\n",
    "        yield item\n",
    "   \n",
    "    hrefs = sel.css('#content > div > div.article > div.paginator > a::attr(\"href\")')\n",
    "    for href in hrefs:\n",
    "        full_url = response.urljoin(href.extract())\n",
    "        yield Request(url=full_url)\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fee5f09",
   "metadata": {},
   "source": [
    "到这里，我们已经可以通过下面的命令让爬虫运转起来。\n",
    "\n",
    "   ```Shell\n",
    "   scrapy crawl movie\n",
    "   ```\n",
    "\n",
    "   可以在控制台看到爬取到的数据，如果想将这些数据保存到文件中，可以通过`-o`参数来指定文件名，Scrapy 支持我们将爬取到的数据导出成 JSON、CSV、XML 等格式。\n",
    "\n",
    "   ```Shell\n",
    "   scrapy crawl moive -o result.json\n",
    "   ```\n",
    "\n",
    "   不知大家是否注意到，通过运行爬虫获得的 JSON 文件中有`275`条数据，那是因为首页被重复爬取了。要解决这个问题，可以对上面的代码稍作调整，不在`parse`方法中解析获取新页面的 URL，而是通过`start_requests`方法提前准备好待爬取页面的 URL，调整后的代码如下所示。\n",
    "\n",
    "   ```Python\n",
    "   import scrapy\n",
    "   from scrapy import Selector, Request\n",
    "   from scrapy.http import HtmlResponse\n",
    "   \n",
    "   from demo.items import MovieItem\n",
    "   \n",
    "   \n",
    "   class DoubanSpider(scrapy.Spider):\n",
    "       name = 'douban'\n",
    "       allowed_domains = ['movie.douban.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_requests(self):\n",
    "    for page in range(10):\n",
    "        yield Request(url=f'https://movie.douban.com/top250?start={page * 25}')\n",
    "   \n",
    "def parse(self, response: HtmlResponse):\n",
    "    sel = Selector(response)\n",
    "    movie_items = sel.css('#content > div > div.article > ol > li')\n",
    "    for movie_sel in movie_items:\n",
    "        item = MovieItem()\n",
    "        item['title'] = movie_sel.css('.title::text').extract_first()\n",
    "        item['score'] = movie_sel.css('.rating_num::text').extract_first()\n",
    "        item['motto'] = movie_sel.css('.inq::text').extract_first()\n",
    "        yield item\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6d72bb",
   "metadata": {},
   "source": [
    "3. 如果希望完成爬虫数据的持久化，可以在数据管道中处理蜘蛛程序产生的`Item`对象。例如，我们可以通过前面讲到的`openpyxl`操作 Excel 文件，将数据写入 Excel 文件中，代码如下所示。\n",
    "\n",
    "   ```Python\n",
    "   import openpyxl\n",
    "   \n",
    "   from demo.items import MovieItem\n",
    "   \n",
    "   \n",
    "   class MovieItemPipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a9c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "    self.wb = openpyxl.Workbook()\n",
    "    self.sheet = self.wb.active\n",
    "    self.sheet.title = 'Top250'\n",
    "    self.sheet.append(('名称', '评分', '名言'))\n",
    "   \n",
    "def process_item(self, item: MovieItem, spider):\n",
    "    self.sheet.append((item['title'], item['score'], item['motto']))\n",
    "    return item\n",
    "   \n",
    "def close_spider(self, spider):\n",
    "    self.wb.save('豆瓣电影数据.xlsx')\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3924ad74",
   "metadata": {},
   "source": [
    "上面的`process_item`和`close_spider`都是回调方法（钩子函数）， 简单的说就是 Scrapy 框架会自动去调用的方法。当蜘蛛程序产生一个`Item`对象交给引擎时，引擎会将该`Item`对象交给数据管道，这时我们配置好的数据管道的`parse_item`方法就会被执行，所以我们可以在该方法中获取数据并完成数据的持久化操作。另一个方法`close_spider`是在爬虫结束运行前会自动执行的方法，在上面的代码中，我们在这个地方进行了保存 Excel 文件的操作，相信这段代码大家是很容易读懂的。\n",
    "\n",
    "   总而言之，数据管道可以帮助我们完成以下操作：\n",
    "\n",
    "   - 清理 HTML 数据，验证爬取的数据。\n",
    "   - 丢弃重复的不必要的内容。\n",
    "   - 将爬取的结果进行持久化操作。\n",
    "\n",
    "4. 修改`settings.py`文件对项目进行配置，主要需要修改以下几个配置。\n",
    "\n",
    "   ```Python\n",
    "   # 用户浏览器\n",
    "   USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'\n",
    "   \n",
    "   # 并发请求数量 \n",
    "   CONCURRENT_REQUESTS = 4\n",
    "   \n",
    "   # 下载延迟\n",
    "   DOWNLOAD_DELAY = 3\n",
    "   # 随机化下载延迟\n",
    "   RANDOMIZE_DOWNLOAD_DELAY = True\n",
    "   \n",
    "   # 是否遵守爬虫协议\n",
    "   ROBOTSTXT_OBEY = True\n",
    "   \n",
    "   # 配置数据管道\n",
    "   ITEM_PIPELINES = {\n",
    "      'demo.pipelines.MovieItemPipeline': 300,\n",
    "   }\n",
    "   ```\n",
    "\n",
    "   > **说明**：上面配置文件中的`ITEM_PIPELINES`选项是一个字典，可以配置多个处理数据的管道，后面的数字代表了执行的优先级，数字小的先执行。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
